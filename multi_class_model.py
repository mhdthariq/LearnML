# -*- coding: utf-8 -*-
"""Multi Class Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GhOrAaLGuQlGt6xW1IG2oAta7rzE4HRs
"""

# Import libraries
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Input, Dropout, BatchNormalization
import matplotlib.pyplot as plt
import tensorflow as tf

# Load the dataset
df = pd.read_csv('https://drive.google.com/uc?id=1roJ83AbgzDcvRr0Gwud0BmdUQx-oSG-w')
df.head()

# Drop the id column
df = df.drop(columns='Id')

# Change categorical data to numeric using 'one hot encoding' method
category = pd.get_dummies(df.Species, dtype=int)
category

new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='Species')
new_df

dataset = new_df.values
dataset

# Select the first 4 columns to use as attributes
X = dataset[:,0:4]
# Select the last 3 columns as the label
y = dataset[:,4:7]

# Normalization
min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)
X_scale

X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y, test_size=0.3)

# Build a Sequential model for multi-class classification
model = Sequential([
    # Input layer: expects 4 input features
    Input(shape=(4,)),

    # First hidden layer: 64 neurons, ReLU activation
    Dense(64, activation='relu'),

    # Second hidden layer: 64 neurons, ReLU activation
    Dense(64, activation='relu'),

    # Use droupout
    Dropout(0.5),

    # Add batch normalization
    BatchNormalization(momentum=0.99),

    # Output layer: 3 neurons (for 3 classes), softmax activation
    Dense(3, activation='softmax')
])

model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Use Callback to stop model if the accuracy is more than 90%
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAccuracy has reached >90%!")
      self.model.stop_training = True
callbacks = myCallback()

hist = model.fit(X_train, Y_train, epochs=100, callbacks = [callbacks])

model.evaluate(X_test, Y_test, batch_size=1)

plt.plot(hist.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(hist.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()