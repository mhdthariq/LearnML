# -*- coding: utf-8 -*-
"""Text Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tzZjL2YSwMbPfGGw2if9E6YIaLBy5d_G

# 1. Case Folding
"""

# Example text
original_text = "This Is An Example Of Text To Be Converted Into Lowercase."

# Change the text to lowercase
lowercase_text = original_text.lower()

# Show the result
print("Original Text:", original_text)
print("Text after being converted to lowercase:", lowercase_text)

# Examples of text with a mixture of uppercase and lowercase letters
original_text = """
This is an example of text with a mix of uppercase and lowercase letters.
This example is used for a case folding demonstration in text pre-processing.
Using case folding, all the letters in the text will be changed to lowercase.
This helps in ensuring consistency in text analysis.
"""

# Change the text to lowercase with case folding
lowercase_text = original_text.lower()

# Show the result
print("Original Text:")
print(original_text)
print("Text after case folding:")
print(lowercase_text)

"""# 2. Removal Special Character

## Remove Number
"""

def remove_number(text):
    text_without_numbers = ''.join([char for char in text if not char.isdigit()])
    return text_without_numbers

# Example text with numbers
text_with_numbers = "This is an example of text with the number 12345 to be removed."

# Call a function to remove numbers
text_without_numbers = remove_number(text_with_numbers)

# Show the result
print("Teks with number:", text_with_numbers)
print("Teks without number:", text_without_numbers)

"""## Remove Punctuation Marks"""

import string

def remove_punctuation(text):
    # Create a set that contains all punctuation marks
    punctuation_set = set(string.punctuation)

    # Remove punctuation from text
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)

    return text_without_punctuation

# Examples of text with multiple punctuation marks
original_text = """
In this world, many things happen, from the small to the big. We can see the beauty, but also the cruelty. There is hope, but also despair. After all, life goes on, no matter what happens!
"""

# Remove punctuation from text
text_without_punctuation = remove_punctuation(original_text)

# Show the result
print("Original text:")
print(original_text)
print("\nText after removing punctuation:")
print(text_without_punctuation)

"""## Removing White Space"""

text = "   This is an example of a sentence with spaces at the beginning and end.    "
text_after_strip = text.strip()
print(text_after_strip)

text_with_whitespace = "This   is   an   example   of   a  sentence   with   spaces   in  it."
text_without_whitespace = text_with_whitespace.replace(" ", "")
print(text_without_whitespace)

"""# 3. Stopword Removal"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt_tab')  # Untuk tokenisasi kata

teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."

# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)

# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))

# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]

# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

print("Teks asli:", teks)
print("Teks setelah filtering stopwords NLTK:", teks_tanpa_stopwords)

"""# 4. Tokenizing

## Word Tokenization
"""

# We want to separate phrases based on comma punctuation (,)
text = "This is an example sentence for the tokenization of the word"
phrases = text.split(' ')
print(phrases)

"""## Sentence Tokenization"""

# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re

text = "Ini adalah contoh kalimat pertama. Dan ini adalah contoh kalimat kedua."
sentences = re.split(r'(?<=[.!?]) +', text)
sentences = re.split(r'(?<=[.!?]) +', text)
print(sentences)

"""## Phrase Tokenization"""

# We want to separate phrases based on comma punctuation (,)
text = "Apples, oranges, bananas, and mangoes."
phrases = text.split(',')
print(phrases)

"""## Rule-based Tokenization"""

# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re

text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

"""## Model-based Tokenization"""

# For example, using spaces as word separators
text = "This is an example of model-based tokenization."
tokens = text.split()
print(tokens)

"""# 5. Stemming"""

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
words = ["running", "easily", "bought", "crying", "leaves"]
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)

"""# 6. Lemmatization"""

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()
words = ["running", "easily", "bought", "crying", "leaves"]

lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]
print(lemmatized_words)