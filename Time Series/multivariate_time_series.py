# -*- coding: utf-8 -*-
"""Multivariate Time Series.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SplnLX0-gieS6O3H17iw6ZcK_gdnwZwz
"""

import pandas as pd
import tensorflow as tf

# Read the data from google drive
df = pd.read_csv('https://drive.google.com/uc?id=1AZRfFoyekqSYpri5183RmJjciRGz_ood', sep=',',
                     index_col='datetime', header=0)

# Show the data
df

# Preprocessing
def normalize_series(data, min, max):
    data = data - min
    data = data / max
    return data
data = df.values
data = normalize_series(data, data.min(axis=0), data.max(axis=0))

# Count the features
N_FEATURES = len(df.columns)
print(N_FEATURES)

# Split data
SPLIT_TIME = int(len(data) * 0.5)
x_train = data[:SPLIT_TIME]
x_valid = data[SPLIT_TIME:]

def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):
    # Create a tf.data.Dataset from the input series
    ds = tf.data.Dataset.from_tensor_slices(series)
    # Print the dataset to inspect its initial structure
    print("Original Dataset:", ds)
    # Create windows of data
    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)
    # Print the windowed dataset
    print("\nWindowed Dataset:", ds)
    # Flatten the windows into batches
    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))
    # Print the flattened dataset
    print("\nFlattened Dataset:", ds)
    # Split the batches into input (past) and target (future)
    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))
    # Print the dataset with input and target
    print("\nDataset with Input and Target:", ds)
    # Batch the dataset and prefetch for performance
    return ds.batch(batch_size).prefetch(1)

# Define parameters for windowed dataset
BATCH_SIZE = 32  # Number of samples in each batch
N_PAST = 24       # Number of past time steps to consider
N_FUTURE = 24      # Number of future time steps to predict
SHIFT = 1         # Number of time steps to shift the window

# Create windowed datasets for training and validation
train_set = windowed_dataset(
    series=x_train,
    batch_size=BATCH_SIZE,
    n_past=N_PAST,
    n_future=N_FUTURE,
    shift=SHIFT
)
valid_set = windowed_dataset(
    series=x_valid,
    batch_size=BATCH_SIZE,
    n_past=N_PAST,
    n_future=N_FUTURE,
    shift=SHIFT
)

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(N_PAST, N_FEATURES)),  # Use Input layer
    tf.keras.layers.Dense(64),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(N_FEATURES)
])

class myCallback(tf.keras.callbacks.Callback):
    """
    Custom callback to stop training when both MAE and accuracy reach thresholds.
    """
    def on_epoch_end(self, epoch, logs={}):
        """
        Checks metrics at the end of each epoch and stops training if conditions are met.
        """
        # Check if both MAE and accuracy are below thresholds
        if (logs.get('mae') < 0.055 and logs.get('val_mae') < 0.055 and
            logs.get('accuracy') > 0.95 and logs.get('val_accuracy') > 0.95):
            # Stop training if conditions are met
            print("\nReached target accuracy and MAE, stopping training!")
            self.model.stop_training = True

# Create an instance of the callback
callbacks = myCallback()

# Prepare the model for training
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss='mae',
                  optimizer= optimizer,
                  metrics=["mae" , "accuracy"])

# Assuming 'history' is a variable to store training history
history = model.fit(
    train_set,
    validation_data=valid_set,  # Removed extra parentheses
    epochs=100,
    callbacks=callbacks,
    verbose=1
)

train_pred = model.predict(train_set)
train_pred[0][0]